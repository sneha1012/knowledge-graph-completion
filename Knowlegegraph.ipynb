{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlyZDCP8DhTMghRX2KhRy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sneha1012/knowledge-graph-completion/blob/main/Knowlegegraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph attention Networks for knowledge completion graphs using Pytorch**\n",
        "\n",
        "\n",
        "Graph Attention Networks (GAT)\n",
        "Graph Attention Networks (GAT) introduce an attention mechanism that allows nodes to focus on their most informative neighbors, rather than treating all neighbors equally. This is particularly useful for graph-structured data where the importance of neighbors can vary significantly.\n",
        "\n",
        "we are using **Freebase 15k-237** Dataset present.\n",
        "**Origin**: Derived from Freebase and contains a subset of the FB15K dataset.\n",
        "**Textual** Mentions: Derived from 200 million sentences from the ClueWeb12 corpus coupled with Freebase entity mention annotations.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wjJsZCbxfaCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision networkx matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n"
      ],
      "metadata": {
        "id": "KJkDQHm5iSia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting the drive on collab and then defining the path to our datset"
      ],
      "metadata": {
        "id": "UAn_zWz1jmCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/Data for GATS/train.txt'\n"
      ],
      "metadata": {
        "id": "r5X-heb1i-2P",
        "outputId": "d14a1346-e60d-4da9-fe90-b5faa63db28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing our data\n",
        "\n",
        "# Dictionaries to map entities and relations to unique integer IDs\n",
        "entity2id = {}\n",
        "relation2id = {}\n",
        "\n",
        "# Lists to store triples\n",
        "triples = []\n",
        "\n",
        "# Read the file and process the data\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        s, p, o = line.strip().split('\\t')\n",
        "\n",
        "        # Assign unique IDs to entities and relations\n",
        "        if s not in entity2id:\n",
        "            entity2id[s] = len(entity2id)\n",
        "        if o not in entity2id:\n",
        "            entity2id[o] = len(entity2id)\n",
        "        if p not in relation2id:\n",
        "            relation2id[p] = len(relation2id)\n",
        "\n",
        "        # Store the triples\n",
        "        triples.append((entity2id[s], relation2id[p], entity2id[o]))\n"
      ],
      "metadata": {
        "id": "CEC44nn1kYd4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.a = nn.Linear(2*out_features, 1, bias=False)\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        h = self.W(input)\n",
        "        N = h.size()[0]\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(self.a(a_input).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Nn7xnlculgaG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.005\n",
        "weight_decay = 5e-4\n",
        "epochs = 200\n",
        "nhid = 8\n",
        "nclass = 7\n",
        "dropout = 0.6\n",
        "alpha = 0.2\n",
        "nheads = 8\n",
        "\n",
        "# Model and optimizer\n",
        "model = GAT(nfeat=initial_entity_embeddings.shape[1],\n",
        "            nhid=nhid,\n",
        "            nclass=nclass,\n",
        "            dropout=dropout,\n",
        "            nheads=nheads,\n",
        "            alpha=alpha)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(initial_entity_embeddings_tensor, adjacency_matrix)  # Define the adjacency_matrix\n",
        "    loss_train = F.nll_loss(output, labels)  # Define the labels\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "zxXvUkDylkha",
        "outputId": "6ef54bfa-c875-4d10-ac23-a59d79c0a929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-029912bdf599>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Model and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = GAT(nfeat=initial_entity_embeddings.shape[1],\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mnhid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnhid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mnclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnclass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'initial_entity_embeddings' is not defined"
          ]
        }
      ]
    }
  ]
}